<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c18{border-right-style:solid;padding:5pt 9pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:38pt;border-top-color:#000000;border-bottom-style:solid}.c21{border-right-style:solid;padding:5pt 6pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:204.5pt;border-top-color:#000000;border-bottom-style:solid}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c24{font-weight:400;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c9{border-spacing:0;border-collapse:collapse;margin-right:auto}.c25{padding-top:0pt;padding-bottom:0pt;line-height:1.0961538461538463;margin-right:75pt}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c19{padding-top:14pt;padding-bottom:4pt;line-height:1.15;text-align:left}.c26{background-color:#ffffff;font-size:1pt;color:#777777}.c11{background-color:#ffffff;font-size:10pt;color:#660099}.c15{background-color:#ffffff;font-size:10pt;color:#222222}.c17{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c13{background-color:#ffffff;font-size:13pt;color:#660099}.c8{background-color:#ffffff;font-size:10pt;color:#006621}.c7{padding-top:1pt;padding-bottom:1pt;line-height:1.209}.c1{color:#1155cc;text-decoration:underline}.c16{font-weight:700;text-decoration:underline}.c0{orphans:2;widows:2}.c2{color:inherit;text-decoration:inherit}.c6{height:11pt}.c3{page-break-after:avoid}.c5{height:0pt}.c12{height:14pt}.c23{font-weight:700}.c10{margin-right:9pt}.c22{line-height:1.209}.c20{text-decoration:underline}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c17"><h3 class="c0 c3" id="h.kysemwr6yyuw"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dsearch_authors%26mauthors%3Djan%2Bsedivy%26hl%3Den%26oi%3Dao&amp;sa=D&amp;ust=1477737172509000&amp;usg=AFQjCNEzr6cqcWDZrIN7CGozkxzRbZC7XA">User profiles for jan sedivy</a></span></h3><a id="t.989793e8eac5041e4e6254479a47749c45012669"></a><a id="t.0"></a><table class="c9"><tbody><tr class="c5"><td class="c18" colspan="1" rowspan="1"><p class="c0 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 40.00px; height: 80.00px;"><img alt="" src="images/image00.png" style="width: 40.00px; height: 80.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td><td class="c21" colspan="1" rowspan="1"><h4 class="c0 c3 c19" id="h.byyxqub5ygzd"><span class="c1 c24"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;ust=1477737172511000&amp;usg=AFQjCNEeM1Au0XvYhgqgQLtgoIad7ESE4A">Jan Sedivy</a></span></h4><p class="c14 c0"><span class="c4">Czech Technical University, dpt. Cybernetics</span></p><p class="c14 c0"><span class="c4">Verified email at fel.cvut.cz</span></p><p class="c14 c0"><span class="c4">Cited by 1586</span></p></td></tr></tbody></table><h3 class="c0 c3 c12" id="h.xa1klchikmta"><span></span></h3><h3 class="c0 c3" id="h.dded05xh3pa5"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US6016476&amp;sa=D&amp;ust=1477737172514000&amp;usg=AFQjCNHZ4SPFYrlO_5bGmZZvPIILrNfy3g">Portable information and transaction processing system and method utilizing biometric authorization and digital certificate security</a></span></h3><p class="c0"><span>SH Maes, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172514000&amp;usg=AFQjCNFZkDT8Pj8qwnG2m24nIJP2VUb6aA">J Sedivy</a></span><span>&nbsp;- US Patent 6,016,476, 2000 - Google Patents</span></p><p class="c0"><span>The present invention is a portable client PDA with a touch screen or other equivalent user </span></p><p class="c0"><span>interface and having a microphone and local central processing unit (CPU) for processing </span></p><p class="c0"><span>voice commands and for processing biometric data to provide user verification. The PDA ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D9991133802222219369%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172516000&amp;usg=AFQjCNFSU6ODl81JVdn1LUDVV0yXC8zssg">Cited by 754</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:aXDaAEKjp4oJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172517000&amp;usg=AFQjCNEgJCjN41cYixAYaIq_ZKecp0oNIw">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D9991133802222219369%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172517000&amp;usg=AFQjCNGtrlZ1nDK1qZJa2S5DietsPf2BvQ">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?hl%3Den%26q%3Djan%2Bsedivy%26btnG%3D%26as_sdt%3D1%252C5%26as_sdtp%3D%23&amp;sa=D&amp;ust=1477737172518000&amp;usg=AFQjCNF5SavfjkA6MNH6IZjhJApnGKwMBg">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:u5HHmVD_uO8C%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172519000&amp;usg=AFQjCNFb8fEwQWvoOE4OPUZPbvAQ7YUnzQ">Saved</a></span></p><h3 class="c0 c3" id="h.7bn7ad8mtoqc"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US20030046316&amp;sa=D&amp;ust=1477737172519000&amp;usg=AFQjCNGGRNc_WkrENOPCDvY-ySgT2NS8cQ">Systems and methods for providing conversational computing via javaserver pages and javabeans</a></span></h3><p class="c0"><span>&hellip;, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DxKxv70cAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172520000&amp;usg=AFQjCNEiy-SEX9BPrbnjwKP5woiUh4iSOQ">J Kleindienst</a></span><span>, S Maes, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DmVNjyD8AAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172521000&amp;usg=AFQjCNGmYc1OvpW_Iv5nuZ-YOyfUzhNSWQ">T Raman</a></span><span>, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172522000&amp;usg=AFQjCNELo5f221u2KUYd1L26sygmMuoJRg">J Sedivy</a></span><span>&nbsp;- US Patent App. 09/ &hellip;, 2001 - Google Patents</span></p><p class="c0"><span>A new application programming language is provided which is based on user interaction </span></p><p class="c0"><span>with any device which a user is employing to access any type of information. The new </span></p><p class="c0"><span>language is referred to herein as a &ldquo;Conversational Markup Language (CML). In a ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D15299250154790635932%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172524000&amp;usg=AFQjCNEE5d0FZXyGJx2_JifN80mFR7HoiQ">Cited by 220</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:nMki9vTaUdQJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172524000&amp;usg=AFQjCNHls_jDRTEhvvPoibiwJmlD98dGdg">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D15299250154790635932%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172525000&amp;usg=AFQjCNGK-JMLMpZQUJb_z1hcVV7wuLUP_Q">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?hl%3Den%26q%3Djan%2Bsedivy%26btnG%3D%26as_sdt%3D1%252C5%26as_sdtp%3D%23&amp;sa=D&amp;ust=1477737172526000&amp;usg=AFQjCNEQKc4NxTnWX_IJF7tEMRqIo7uFKQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:u-x6o8ySG0sC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172526000&amp;usg=AFQjCNGE_hY2hOGhtPwZ1wqkRTH0Ijvz6w">Saved</a></span></p><h3 class="c0 c3" id="h.wj41idcb8hs8"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US6442519&amp;sa=D&amp;ust=1477737172527000&amp;usg=AFQjCNFty_3KiB9055DWoag33BAe26W_vw">Speaker model adaptation via network of similar users</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DhDnjeooAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172528000&amp;usg=AFQjCNEJQ6J_WQqQeGjbzjMZVJvGna5tYA">D Kanevsky</a></span><span>, VV Libal, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172528000&amp;usg=AFQjCNGMzoYgi5stl81Xae5QPdH-ahlAOg">J Sedivy</a></span><span>&hellip; - US Patent 6,442,519, 2002 - Google Patents</span></p><p class="c0"><span>A speech recognition system, method and program product for recognizing speech input </span></p><p class="c0"><span>from computer users connected together over a network of computers. Speech recognition </span></p><p class="c0"><span>computer users on the network are clustered into classes of similar users according their ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D303674987309033139%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172530000&amp;usg=AFQjCNHNs_or_Kdq014jWfTSIeGH1zvk4g">Cited by 124</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:s-a5wcreNgQJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172531000&amp;usg=AFQjCNHdrvQx5HeupKxmYuHQ_CgwfngeLg">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D303674987309033139%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172531000&amp;usg=AFQjCNGhEa1UCvXQy5uCe6gKaADwGctYzw">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?hl%3Den%26q%3Djan%2Bsedivy%26btnG%3D%26as_sdt%3D1%252C5%26as_sdtp%3D%23&amp;sa=D&amp;ust=1477737172532000&amp;usg=AFQjCNGFBPPq7PhYtmbcfwp3lgsjSSF2jQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:d1gkVwhDpl0C%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172532000&amp;usg=AFQjCNGTBWOdCOZnemDItr_hkK5YgfXOrA">Saved</a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?hl%3Den%26q%3Djan%2Bsedivy%26btnG%3D%26as_sdt%3D1%252C5%26as_sdtp%3D%23&amp;sa=D&amp;ust=1477737172534000&amp;usg=AFQjCNEVm2a_QN9YbQ_sIiLZ5HIOm5K8wA"></a></span></p><h3 class="c0 c3" id="h.jpim58ama1pc"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US5835888&amp;sa=D&amp;ust=1477737172534000&amp;usg=AFQjCNGrOmN1pis3DfXny1oAKQorpjImzw">Statistical language model for inflected languages</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DhDnjeooAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172535000&amp;usg=AFQjCNErTgdjwWBXb9idv5DGMm6_xE4OoA">D Kanevsky</a></span><span>, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3D1S7VwIcAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172536000&amp;usg=AFQjCNG1Wh_FXM3CUkF0O4UaY-i96pZ3-A">SE Roukos</a></span><span>, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172537000&amp;usg=AFQjCNF92CEji_XXDPIGBlEiAhagcPy99A">J Sedivy</a></span><span>&nbsp;- US Patent 5,835,888, 1998 - Google Patents</span></p><p class="c0"><span>A statistical language model for inflected languages, having very large vocabularies, is </span></p><p class="c0"><span>generated by splitting words into stems, prefixes and endings, and deriving trigrams for the </span></p><p class="c0"><span>stems, ending and prefixes. The statistical dependence of endings and prefixes from each ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D8576097010921912873%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172538000&amp;usg=AFQjCNFa-IsmQH15SR3RCaI0D2V0M0MXmA">Cited by 68</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:KTpKF8tqBHcJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172539000&amp;usg=AFQjCNGqSE4xguqJx4XVzplGcv8reAv5_w">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D8576097010921912873%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172540000&amp;usg=AFQjCNHKUriGCPwELz2I-TcIikQscSdZ-w">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?hl%3Den%26q%3Djan%2Bsedivy%26btnG%3D%26as_sdt%3D1%252C5%26as_sdtp%3D%23&amp;sa=D&amp;ust=1477737172540000&amp;usg=AFQjCNFGyZGSvDe0XZuXBl__lxD_XEwyxA">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:9yKSN-GCB0IC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172541000&amp;usg=AFQjCNHmKNnUNr8HltW5grnLmia8woqUzg">Saved</a></span></p><h3 class="c0 c3" id="h.8f8o1795h826"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US7315613&amp;sa=D&amp;ust=1477737172541000&amp;usg=AFQjCNHepp9Sk9V8Ozuv37mavw7Y5mWQ2A">Multi-modal messaging</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DxKxv70cAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172542000&amp;usg=AFQjCNH3svfgozw5Rwya2rmJhb_rKgWcAg">J Kleindienst</a></span><span>, M Labsky, SH Maes, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172543000&amp;usg=AFQjCNFs-yA1cj878KSA4BKVxvHra4R6PA">J Sedivy</a></span><span>&nbsp;- US Patent 7,315,613, 2008 - Google Patents</span></p><p class="c0"><span>Systems and methods for multi-modal messaging that enable a user to compose, send and </span></p><p class="c0"><span>retrieve messages, such as SMS, MMS, IM or ordinary e-mail messages, for example, using </span></p><p class="c0"><span>one or more I/O (input/output) modalities (eg, speech I/O and/or GUI I/O). A method for ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D7148994282512115933%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172545000&amp;usg=AFQjCNHrUk7pScwzeOrsD9BSqAaPUL1-pg">Cited by 53</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:3ehuRmxUNmMJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172546000&amp;usg=AFQjCNEboXeht7EQvStsh9-K1KS5RTq2xg">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D7148994282512115933%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172547000&amp;usg=AFQjCNHG8fZ_-r1vmpbo54tpkebDDVSbMQ">All 4 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?hl%3Den%26q%3Djan%2Bsedivy%26btnG%3D%26as_sdt%3D1%252C5%26as_sdtp%3D%23&amp;sa=D&amp;ust=1477737172548000&amp;usg=AFQjCNHZcBA3NCWJDzuL1JJBD8WMrLb1Ig">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:IjCSPb-OGe4C%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172548000&amp;usg=AFQjCNGt3vf3Iqx1EzIun8Zd9Nrz9ZA_Yg">Saved</a></span></p><h3 class="c0 c3" id="h.yj0pac6rcw5v"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US7487440&amp;sa=D&amp;ust=1477737172549000&amp;usg=AFQjCNHkZtSf1Etp3RUfadBWSOoX_OGVEQ">Reusable voiceXML dialog components, subdialogs and beans</a></span></h3><p class="c0"><span>&hellip;, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DxKxv70cAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172550000&amp;usg=AFQjCNGt6rqPOJP7pROAZaZbBIevaFKZ8Q">J Kleindienst</a></span><span>, SH Maes, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DmVNjyD8AAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172551000&amp;usg=AFQjCNFbhCKoOXwqpBRSQOgARTNPbQPQxA">TV Raman</a></span><span>, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172551000&amp;usg=AFQjCNFG0l7QZDhXteDKqMvbcGzJq2S-gA">J Sedivy</a></span><span>&hellip; - US Patent &hellip;, 2009 - Google Patents</span></p><p class="c0"><span>Systems and methods for building speech-based applications using reusable dialog </span></p><p class="c0"><span>components based on VoiceXML (Voice eXtensible Markup Language). VoiceXML reusable </span></p><p class="c0"><span>dialog components can be used for building a voice interface for use with multi-modal, ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D15159368413347716682%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172553000&amp;usg=AFQjCNH13qrP3j61H7PJJlXS-w6HR33fNQ">Cited by 48</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:Sh7eEEHlYNIJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172554000&amp;usg=AFQjCNHO0r0mFDcmIm0UzpH-uPO6iEnmQA">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D15159368413347716682%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172554000&amp;usg=AFQjCNEGGTYrr9ydr109EgycjYC4q3nB8g">All 4 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?hl%3Den%26q%3Djan%2Bsedivy%26btnG%3D%26as_sdt%3D1%252C5%26as_sdtp%3D%23&amp;sa=D&amp;ust=1477737172555000&amp;usg=AFQjCNHQ_RLAfh7nigjqjM3C3U9ePEv0Gw">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:WF5omc3nYNoC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172556000&amp;usg=AFQjCNFxbzCMRYWw9zrGyfHbK_FF5Fq9gg">Saved</a></span></p><h3 class="c0 c3" id="h.vf7z6uqmr7w"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US6023673&amp;sa=D&amp;ust=1477737172557000&amp;usg=AFQjCNGZpZIeVGWcGpVUlUAWk0aQC691QQ">Hierarchical labeler in a speech recognition system</a></span></h3><p class="c0"><span>R Bakis, D Nahamoo, MA Picheny, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172557000&amp;usg=AFQjCNHa405hjhB6RLw2xoeiYumMQUpvRg">J Sedivy</a></span><span>&nbsp;- US Patent 6,023,673, 2000 - Google Patents</span></p><p class="c0"><span>A speech coding apparatus and method uses a hierarchy of prototype sets to code an </span></p><p class="c0"><span>utterance while consuming fewer computing resources. The value of at least one feature of </span></p><p class="c0"><span>an utterance is measured during each of a series of successive time intervals to produce a ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D10295290690256349005%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172559000&amp;usg=AFQjCNGDjZZE1T_M4uvPwYAX950_XcqhKg">Cited by 44</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:TacdBFY44I4J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172560000&amp;usg=AFQjCNFYzgb1UuYwV21Va4im-qBUkR0AAA">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D10295290690256349005%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172561000&amp;usg=AFQjCNFbOja-Qe-tgTfYSPdQR-Poan558A">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?hl%3Den%26q%3Djan%2Bsedivy%26btnG%3D%26as_sdt%3D1%252C5%26as_sdtp%3D%23&amp;sa=D&amp;ust=1477737172562000&amp;usg=AFQjCNFgKmjZgA-MJQsRHYZ-DjnRJMCCSg">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:qjMakFHDy7sC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172563000&amp;usg=AFQjCNH8WUTPYlwYucOz5dvsHX7_bJ1VYA">Saved</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.researchgate.net/profile/Liam_Comerford/publication/3908353_The_IBM_Personal_Speech_Assistant/links/02e7e52a89ec003d29000000.pdf&amp;sa=D&amp;ust=1477737172564000&amp;usg=AFQjCNHvcGfjyU9GDyssNifyEyj91rdYkQ">[PDF] researchgate.net</a></span></p><h3 class="c0 c3" id="h.9xlyi18xvgzk"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber%3D940752&amp;sa=D&amp;ust=1477737172565000&amp;usg=AFQjCNH2HHrSS9OPPA35dmOWfytcVgJghQ">The IBM personal speech assistant</a></span></h3><p class="c0"><span>&hellip;, R Gopinath, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172565000&amp;usg=AFQjCNGE0D947oAhcroShtnK2-lYwTH86w">J Sedivy</a></span><span>&nbsp;- &hellip; , Speech, and Signal &hellip;, 2001 - ieeexplore.ieee.org</span></p><p class="c0"><span>ABSTRACT In this paper, we describe technology and experience with an experimental </span></p><p class="c0"><span>personal information manager, which interacts with the user primarily but not exclusively </span></p><p class="c0"><span>through speech recognition and synthesis. This device, which controls a client PDA, is ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D11356420923350539544%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172567000&amp;usg=AFQjCNEjTkUxh2k9zqfhkatesIaFdBTvnQ">Cited by 43</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:GF0fMKkcmp0J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172568000&amp;usg=AFQjCNFmOGbD1V63HvbrufvPuq8cIS2vEQ">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D11356420923350539544%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172569000&amp;usg=AFQjCNGrBzkxSrk163he4oD_W7zUdwcDeQ">All 10 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?hl%3Den%26q%3Djan%2Bsedivy%26btnG%3D%26as_sdt%3D1%252C5%26as_sdtp%3D%23&amp;sa=D&amp;ust=1477737172569000&amp;usg=AFQjCNHEDJ8mF7T7WUzfkGl65UyyYziq0g">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:2osOgNQ5qMEC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172570000&amp;usg=AFQjCNHalxe7aI3Ndo05WiFY3zAYSG0teg">Saved</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://www.pnas.org/content/101/49/17011.full&amp;sa=D&amp;ust=1477737172571000&amp;usg=AFQjCNEueHcKbW38ovaomRDKQWUFDx6GXA">[HTML] pnas.org</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?output%3Dinstlink%26q%3Dinfo:wXZgXm1fPNgJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5%26scillfp%3D9611602791854933968%26oi%3Dlle&amp;sa=D&amp;ust=1477737172572000&amp;usg=AFQjCNGwZ4zXKxUY10B1cvjxBMHr4y3NBQ">Full View</a></span></p><h3 class="c0 c3" id="h.t6jg3fs4kp63"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US6073091&amp;sa=D&amp;ust=1477737172573000&amp;usg=AFQjCNFGXyiaVUzWphqorFa1Fdty_Jitew">Apparatus and method for forming a filtered inflected language model for automatic speech recognition</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DhDnjeooAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172574000&amp;usg=AFQjCNElWeeNdNuKyPZEwUg_niq-VC2NkA">D Kanevsky</a></span><span>, MD Monkowski, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172575000&amp;usg=AFQjCNGEOkLh4VLi7_6nlyP7Oiqimtbf2A">J Sedivy</a></span><span>&nbsp;- US Patent 6,073,091, 2000 - Google Patents</span></p><p class="c0"><span>A method of forming a language model for a language having a selected vocabulary of word </span></p><p class="c0"><span>forms comprises:(a) mapping the word forms into integer vectors in accordance with </span></p><p class="c0"><span>frequencies of word form occurrence;(b) partitioning the integer vectors into subsets, the ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D13216034470874831981%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172577000&amp;usg=AFQjCNH3zbJKcJ8sLAbKVOYYEs_XIUlMCw">Cited by 24</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:bdzPZ1TJaLcJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172578000&amp;usg=AFQjCNE4jThndrReXmHjMd8wXJ4D2wG9Dw">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D13216034470874831981%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172578000&amp;usg=AFQjCNGvlFTnRFLcUkACfAkpJtS0dMlm5Q">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D10%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172580000&amp;usg=AFQjCNHtFcZUMLPrLEvFEYB4ioL8xnYpcQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D10%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:Tyk-4Ss8FVUC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172581000&amp;usg=AFQjCNG2yXMethKE4XTFc3pN9pCQogUQ-g">Saved</a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D10%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172582000&amp;usg=AFQjCNEVzHQJEUnKfsWk-48TmkpUGLGdmw"></a></span></p><h3 class="c0 c3" id="h.lr1m707aquv7"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US5522011&amp;sa=D&amp;ust=1477737172582000&amp;usg=AFQjCNEFM5drUhjtvalUtQ9TcEIv6YLkeg">Speech coding apparatus and method using classification rules</a></span></h3><p class="c0"><span>&hellip;, D Nahamoo, MA Picheny, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172583000&amp;usg=AFQjCNHryC1B3SqOrBYlgdL8SNC9SyZ-Bw">J Sedivy</a></span><span>&nbsp;- US Patent &hellip;, 1996 - Google Patents</span></p><p class="c0"><span>A speech coding apparatus and method uses classification rules to code an utterance while </span></p><p class="c0"><span>consuming fewer computing resources. The value of at least one feature of an utterance is </span></p><p class="c0"><span>measured during each of a series of successive time intervals to produce a series of ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D12315501776948038246%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172585000&amp;usg=AFQjCNGIQ-HiOX7W0CycSpdZ-qhE21UkrA">Cited by 21</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:ZgqupZ1z6aoJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172586000&amp;usg=AFQjCNEaAIewBld-d2JALy-450N6nAn7pw">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D12315501776948038246%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172586000&amp;usg=AFQjCNFZzZj5kGcjaGduFc5zxFGKJmibvw">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D10%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172587000&amp;usg=AFQjCNEX6XyR2C4Vjn4MgvZjnpE4Gis-cg">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D10%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:zYLM7Y9cAGgC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172588000&amp;usg=AFQjCNGQDLDQI5nfGzzoh5yjPgVcDgMbuA">Saved</a></span></p><h3 class="c0 c3" id="h.vgoda8t4xsz1"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US7478171&amp;sa=D&amp;ust=1477737172589000&amp;usg=AFQjCNHH04zDIlGdvEHtPvgxXQigtspFjQ">Systems and methods for providing dialog localization in a distributed environment and enabling conversational communication using generalized user gestures</a></span></h3><p class="c0"><span>GN Ramaswamy, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172590000&amp;usg=AFQjCNGRp1Ul2S027TGOoUpQ4tqVCKhDkA">J Sedivy</a></span><span>, SH Maes - US Patent 7,478,171, 2009 - Google Patents</span></p><p class="c0"><span>Systems and methods for providing conversational computing in a distributed multi-modal </span></p><p class="c0"><span>environment and in particular, systems and methods for enabling a user to conversationally </span></p><p class="c0"><span>communicate with entities in a distributed network using a portable access device, ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D6743951539375440483%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172592000&amp;usg=AFQjCNHDiQN6xVi99C-QYM0FcTdoubIYRQ">Cited by 19</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:YwLsrjFUl10J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172593000&amp;usg=AFQjCNE-10tDnHpSjr8XELPAYLyWb55lEQ">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D6743951539375440483%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172593000&amp;usg=AFQjCNHshZRgEM1ro4PSwZVmSwGWznCyRw">All 4 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D10%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172594000&amp;usg=AFQjCNECPVgAcnLXvL_tZrFXyH0oKrTcKQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D10%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:LkGwnXOMwfcC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172595000&amp;usg=AFQjCNFfGZN4ajMqdY8QOHFKhLLfoIFVFA">Saved</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.21.2748%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;ust=1477737172595000&amp;usg=AFQjCNFAqhq5pBFrpFEOvFgVkS1uxV020A">[PDF] psu.edu</a></span></p><h3 class="c0 c3" id="h.l6taflfro1m3"><span>[PDF] </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.21.2748%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;ust=1477737172596000&amp;usg=AFQjCNG5Tzxyaa83Vtq2atACaRA_3bLgqw">Low-resource speech recognition of 500-word vocabularies</a></span></h3><p class="c0"><span>&hellip;, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DjJy_rQsAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172597000&amp;usg=AFQjCNEbyBr8kc1zOUVJAjT0KMAlhUtCoA">B Maison</a></span><span>, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DjUjfnB0AAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172598000&amp;usg=AFQjCNEaCcCjXVR2XadGBUfESvX6jZ24YA">P Olsen</a></span><span>, H Printz, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172598000&amp;usg=AFQjCNHggSJnLNQyrbkGKa_krJ19nsAPcg">J Sedivy</a></span><span>&nbsp;- Proc. Eurospeech &hellip;, 2001 - Citeseer</span></p><p class="c0"><span>Abstract We describe techniques for enhancing the accuracy, efficiency and features of a </span></p><p class="c0"><span>low-resource, medium-vocabulary, grammarbased speech recognition system. Among the </span></p><p class="c0"><span>issues and techniques we explore are front-end speech/silence detection to reduce ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D3161663663808456381%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172600000&amp;usg=AFQjCNEYwedkNNfi4VRbsT-Ini9al7jsIg">Cited by 19</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:vSaQ3Fl84CsJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172601000&amp;usg=AFQjCNGkDLZEnryZLiJSq5796EQs200dAQ">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D3161663663808456381%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172601000&amp;usg=AFQjCNEsUsR1Kqvt827lsdEJd6U5bL1dLA">All 12 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D10%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172602000&amp;usg=AFQjCNGJr4R-I3TsbMPe7AOuaCrBFz9A6g">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D10%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:Y0pCki6q_DkC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172603000&amp;usg=AFQjCNFXYAXO9RZ0JLIstRwkzcnvXTskNQ">Saved</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D10%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172603000&amp;usg=AFQjCNGlcdSOal2lPTRQMcWjLWvx9B3Sng">More</a></span></p><h3 class="c0 c3" id="h.6znz46umsd0z"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://link.springer.com/chapter/10.1007/978-3-540-30120-2_34&amp;sa=D&amp;ust=1477737172604000&amp;usg=AFQjCNG2uoC-8h1F_ZipEIcy_B_wDkpk1Q">Embedded viavoice</a></span></h3><p class="c0"><span>T Beran, V Bergl, R Hampl, P Krbec, J &Scaron;ediv&yacute;&hellip; - &hellip; Conference on Text, &hellip;, 2004 - Springer</span></p><p class="c0"><span>Abstract In this paper we present IBM Embedded ViaVoice (EVV), a speech recognizer for </span></p><p class="c0"><span>embedded devices. It is designed for grammar-based command and control applications </span></p><p class="c0"><span>with medium to large vocabularies. We show what algorithms and technologies were used ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D17832948062197887226%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172605000&amp;usg=AFQjCNFEmz4EtOWp2X7LNDrKCIjGuX_YCw">Cited by 12</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:-rBLkcdbe_cJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172606000&amp;usg=AFQjCNGZVYLhim4atj3giuSLQUflFRLs3Q">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D17832948062197887226%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172607000&amp;usg=AFQjCNHbAyP3Ayw872ska7zFdryISVMLZg">All 7 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D10%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172607000&amp;usg=AFQjCNEGKtDBYCO6u1coMGikore7wQbsxA">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D10%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172608000&amp;usg=AFQjCNEp02yuh1TSmmIBl2VKH2lnXpp20A">Save</a></span></p><h3 class="c0 c3" id="h.4z6v7x202uze"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US6928404&amp;sa=D&amp;ust=1477737172608000&amp;usg=AFQjCNFYxtbweNiuu8VrN8lRyUrOy719IQ">System and methods for acoustic and language modeling for automatic speech recognition with large vocabularies</a></span></h3><p class="c0"><span>&hellip;, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DhDnjeooAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172609000&amp;usg=AFQjCNFL8BYR1u9psEeemvyWA8_FOVn6yQ">D Kanevsky</a></span><span>, MD Monkowski, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172610000&amp;usg=AFQjCNH20oqznfkci-qPiNn_Wwv9mAvjVg">J Sedivy</a></span><span>&nbsp;- US Patent &hellip;, 2005 - Google Patents</span></p><p class="c0"><span>Systems and methods are provided for generating a language component vocabulary VC for </span></p><p class="c0"><span>a speech recognition system having a language vocabulary V of a plurality of word forms. </span></p><p class="c0"><span>One method for generating a language component vocabulary VC for a speech ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D11285277340700432841%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172612000&amp;usg=AFQjCNFgDiOg1a6l30eInIZwqVVGUovuUw">Cited by 12</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:ydk2DPNbnZwJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172612000&amp;usg=AFQjCNFebriJbW8TNMMkc2eSVTo0IUQe2w">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D11285277340700432841%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172613000&amp;usg=AFQjCNE8CmvcThs88Uxmx4xQbzh7ZOrzXA">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D10%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172613000&amp;usg=AFQjCNEVXafds7oJ55wD7BS05pgGtgHNKg">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D10%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:ufrVoPGSRksC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172614000&amp;usg=AFQjCNHTCan50yaq5ysKb5-dm6bm4mZP0Q">Saved</a></span></p><p class="c0 c6"><span></span></p><h3 class="c0 c3" id="h.tu456bvifiet"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US6862454&amp;sa=D&amp;ust=1477737172615000&amp;usg=AFQjCNEBqsKbGtRyCmRVunIXPl0O7b_xQA">Efficient communication with passive devices</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DhDnjeooAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172616000&amp;usg=AFQjCNFLKSQ1WCFl64FcCPbGuf4uVmkpqQ">D Kanevsky</a></span><span>, M Sabath, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172616000&amp;usg=AFQjCNHed63yTNkJQuenQ6YkzWg0ABz-Og">J Sedivy</a></span><span>, A Zlatsin - US Patent 6,862,454, 2005 - Google Patents</span></p><p class="c0"><span>A system and method that provides data messages to a passive device. A passive device, </span></p><p class="c0"><span>for example watch, is registered together with the telephone number of a cellular telephone </span></p><p class="c0"><span>of a subscriber to the data message service. Since the cellular telephone periodically ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D2272333561771635220%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172618000&amp;usg=AFQjCNGck_f5-QbRuxe8FkF7zNkqSxc3ng">Cited by 11</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:FLKYyFXziB8J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172619000&amp;usg=AFQjCNGW29S5tz3wLsyOvcKID9tvIAjHYA">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D2272333561771635220%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172620000&amp;usg=AFQjCNGqpc_GoJpWnVGU8cuItNv7kEpHBg">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D20%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172620000&amp;usg=AFQjCNH2rcF4dKbT_lcXUqJLtmn8WzeAQg">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D20%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:hqOjcs7Dif8C%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172621000&amp;usg=AFQjCNHNjDccAruwEiKItiIPU289ZbzPnQ">Saved</a></span></p><h3 class="c0 c3" id="h.io9ahta1ql5x"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US6965773&amp;sa=D&amp;ust=1477737172622000&amp;usg=AFQjCNGqDlW-d2EYSDE3esujudMXjhfnXA">Virtual cooperative network formed by local clients in zones without cellular services</a></span></h3><p class="c0"><span>SH Basson, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DhDnjeooAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172622000&amp;usg=AFQjCNEr2gx59-bLd0yAx8oiIjwl3ThjZw">D Kanevsky</a></span><span>, M Sabath, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172623000&amp;usg=AFQjCNFKmoUkMiLQ9H6ywsm12C_AdLastA">J Sedivy</a></span><span>&hellip; - US Patent &hellip;, 2005 - Google Patents</span></p><p class="c0"><span>A method and system for forming a virtual network. The method comprises the steps of </span></p><p class="c0"><span>providing each of a plurality of mobile objects with a transceiver, and transmitting a </span></p><p class="c0"><span>cellular/radio signals from a source. At least a first of the mobile objects is located in an ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D2140073985976414501%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172625000&amp;usg=AFQjCNHhMnFdnua2EGwxSXfRGAR6q8y7Hw">Cited by 10</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:JbEtn_MRsx0J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172626000&amp;usg=AFQjCNH5heqxsx2QEoMZhRnVpxE0y3Xd_w">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D2140073985976414501%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172626000&amp;usg=AFQjCNFhiVuOkCy42ZnnnuJQDWCbCs9eMA">All 4 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D20%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172627000&amp;usg=AFQjCNGrdWLq5vHzZO03Fo5L3JP3Z7zl6A">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D20%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:Se3iqnhoufwC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172628000&amp;usg=AFQjCNE-G4s7pgkN48bDyDy9RJK2atkBBA">Saved</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.8.1501%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;ust=1477737172628000&amp;usg=AFQjCNGKN7bu2Maj9f8YL3BB1QUgXWTAlg">[PDF] psu.edu</a></span></p><h3 class="c0 c3" id="h.u98ii3du5p9k"><span>[PDF] </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.8.1501%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;ust=1477737172629000&amp;usg=AFQjCNEoJ5oXEVAQXDCvK5sISYZ-CsEHOA">Efficient hierarchical labeler algorithm for Gaussian likelihoods computation in resource constrained speech recognition systems</a></span></h3><p class="c0"><span>M Novak, RA Gopinath, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172630000&amp;usg=AFQjCNFzEPnUkGQAcGGfRMYpgUeEMmiSKw">J Sedivy</a></span><span>&nbsp;- available on-line at: http://www. research &hellip;, 2002 - Citeseer</span></p><p class="c0"><span>ABSTRACT This paper presents a new time/memory-efficient algorithm for the evaluation of </span></p><p class="c0"><span>state likelihoods in an HMM-based speech recognizer where the states are modeled by </span></p><p class="c0"><span>Gaussian Mixtures. We first present a fast hierarchical labeling scheme and then an ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D15110007871733460309%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172632000&amp;usg=AFQjCNETAdyrdKUAeRkr2sCYCcRWzjE-7g">Cited by 10</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:VW0HZBqIsdEJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172632000&amp;usg=AFQjCNHyxL0am4Y4TE0sp0xAEHHvzJhPqQ">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D15110007871733460309%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172633000&amp;usg=AFQjCNF5IBVn63MLr06J1vA5D_3FSbKN6Q">All 3 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D20%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172634000&amp;usg=AFQjCNFaPkLdT-39SncxgfpkQRsXad1AyQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D20%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:_FxGoFyzp5QC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172634000&amp;usg=AFQjCNEZmvlmm1qMgh4rHmKxCJLGF03U6w">Saved</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D20%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172635000&amp;usg=AFQjCNEvKDn9KMtodH0XA1axRcHTYid_tQ">More</a></span></p><p class="c0 c6"><span></span></p><h3 class="c0 c3" id="h.6esw3s3l9v4u"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US6974081&amp;sa=D&amp;ust=1477737172636000&amp;usg=AFQjCNHDb8I4AMyCaecSLXiUx75AaROS3w">Smart book</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DhDnjeooAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172637000&amp;usg=AFQjCNEnEYnbyXTdu2-fxwf9CvABIl1iSg">D Kanevsky</a></span><span>, M Sabath, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172637000&amp;usg=AFQjCNGI2HmeBKJL7FyERw-9GI_bMxr9kA">J Sedivy</a></span><span>, A Zlatsin - US Patent 6,974,081, 2005 - Google Patents</span></p><p class="c0"><span>A method and system that permits the purchase of a license to make a limited number of </span></p><p class="c0"><span>copies of a book. At the time of purchase, the purchaser or user is given a key that contains </span></p><p class="c0"><span>the ability to obtain the limited number of copies on demand. The key contains a web ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D14502703716391133750%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172639000&amp;usg=AFQjCNGYFLqgXCjiYL-EJtluYE9ZdeY0ew">Cited by 6</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:NiI7AjH0Q8kJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172640000&amp;usg=AFQjCNHN4CruyDGtlucBrGV_cswGVNMMeA">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D14502703716391133750%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172640000&amp;usg=AFQjCNHM4eUvOWXWQD-ijjaHsNTXISvNYg">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D30%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172641000&amp;usg=AFQjCNHyGFfxgTL8W6rE84x-EkcYGvkizQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D30%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:UebtZRa9Y70C%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172642000&amp;usg=AFQjCNG7MOPclbMX_o2wVn07g1QBeEUxVg">Saved</a></span></p><h3 class="c0 c3" id="h.u81d0t7yaqbk"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US5544277&amp;sa=D&amp;ust=1477737172642000&amp;usg=AFQjCNF55hH8S0P0gZ4R241H_7aB72vbdQ">Speech coding apparatus and method for generating acoustic feature vector component values by combining values of the same features for multiple time intervals</a></span></h3><p class="c0"><span>&hellip;, AJ Nadas, D Nahamoo, MA Picheny, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172643000&amp;usg=AFQjCNEacbOR8bp2eQbHnyL8mjLXNn4pxQ">J Sedivy</a></span><span>&nbsp;- US Patent &hellip;, 1996 - Google Patents</span></p><p class="c0"><span>A speech coding apparatus and method measures the values of at least first and second </span></p><p class="c0"><span>different features of an utterance during each of a series of successive time intervals. For </span></p><p class="c0"><span>each time interval, a feature vector signal has a first component value equal to a first ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D10772460610184919577%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172645000&amp;usg=AFQjCNH8Hwq2ZkwIIad9obszpI1eexGy0A">Cited by 6</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:Gbr1mtl3f5UJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172646000&amp;usg=AFQjCNGWK_H73qOOmXv1bDLutIqpZDHI0w">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D10772460610184919577%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172647000&amp;usg=AFQjCNFuHVwXnF-okPkH7SydJHRBhStpXA">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D30%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172647000&amp;usg=AFQjCNEOtpOfLWMvzaYW-yc8ZjXJc6-9iA">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D30%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:roLk4NBRz8UC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172648000&amp;usg=AFQjCNGTvLUVIW7fiM25WAUDFme3uqNazw">Saved</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://onlinelibrary.wiley.com/doi/10.1111/j.1582-4934.2009.00931.x/full&amp;sa=D&amp;ust=1477737172648000&amp;usg=AFQjCNHWaLWgQ3TFjAdTEjzjrcmz1Mxp4g">[HTML] wiley.com</a></span></p><p class="c0 c6"><span></span></p><h3 class="c0 c3" id="h.168pcuzt8ok"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://www.sciencedirect.com/science/article/pii/S026288560600285X&amp;sa=D&amp;ust=1477737172650000&amp;usg=AFQjCNEsmQ-srBUML84wW-DbMthDSWKaiw">Interaction framework for home environment using speech and vision</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DxKxv70cAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172650000&amp;usg=AFQjCNH6FW293rTLfzjqznxmpucBib0PbQ">J Kleindienst</a></span><span>, T Macek, L Ser&eacute;di, J &Scaron;ediv&yacute; - Image and Vision Computing, 2007 - Elsevier</span></p><p class="c0"><span>In this article we describe an interaction framework that uses speech recognition and </span></p><p class="c0"><span>computer-vision to model new generation of interfaces in the residential environment. We </span></p><p class="c0"><span>outline the blueprints of the architecture and describe the main building blocks. We show ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D13350629597982001602%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172652000&amp;usg=AFQjCNF4kfCS1Dk2rPbXkJZaUXU4aLbWGQ">Cited by 4</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:womIe-L2RrkJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172653000&amp;usg=AFQjCNFZ1aqccZHKxsSDR9K4l_E6Aovwdg">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D13350629597982001602%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172653000&amp;usg=AFQjCNFgx42BdBG88xxi2p7HtioHOolOVQ">All 3 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D40%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172654000&amp;usg=AFQjCNGag3Q6aE6m1_k2t0WPG93f03CDOQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D40%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172655000&amp;usg=AFQjCNHdeBJtY7btRD4JfHLOo4uM5mxgBQ">Save</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.107.3059%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;ust=1477737172656000&amp;usg=AFQjCNGR-25qoPO6maka1hEa75QCJzqa6Q">[PDF] psu.edu</a></span></p><h3 class="c0 c3" id="h.w48rv75dl68a"><span>[PDF] </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.107.3059%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;ust=1477737172657000&amp;usg=AFQjCNHfOGPNrK3VizxqODV_6b_v2285Iw">Vision-enhanced multi-modal interactions in domotic environments</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DxKxv70cAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172657000&amp;usg=AFQjCNGWYJn15KvtkefmRxxta2UCRPoxrg">J Kleindienst</a></span><span>, T Macek, L Ser&eacute;di, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172658000&amp;usg=AFQjCNHrzVt0kFJ28w50MKV8ovwFWW4DdA">J Sedivy</a></span><span>&nbsp;- IBM Tecnolog&iacute;as de Voz y &hellip;, 2004 - Citeseer</span></p><p class="c0 c6"><span></span></p><p class="c0"><span>Abstract. This paper introduces key components of user interaction framework that harness </span></p><p class="c0"><span>speech recognition, multi-modality and computer vision in the residential environment. The </span></p><p class="c0"><span>interface technologies presented are being developed as part of &ldquo;HomeTalk&rdquo; IST-2001- ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D11512316949772500404%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172661000&amp;usg=AFQjCNH3_CpHlOQVpTyWO1uJ8x6CYWbKrA">Cited by 21</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:tP3lXEX3w58J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172662000&amp;usg=AFQjCNF-TysGS3JmDzRG4NiZWJzN9xMfeA">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D11512316949772500404%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172663000&amp;usg=AFQjCNFxsDHOJyTMzim9xGNTwQ1_ZfBe0w">All 4 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D40%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172663000&amp;usg=AFQjCNFh_BjGxY-N6naqr35I_OxkrB9UcQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D40%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:YsMSGLbcyi4C%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172664000&amp;usg=AFQjCNH3nOpo8j3NNyJGoblauvpvtZdmoQ">Saved</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D40%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172665000&amp;usg=AFQjCNGRi9OjazdQrQM9AdQSEFjIfDL6HQ">More</a></span></p><h3 class="c0 c3" id="h.oycjcy8pqn5p"><span>[PDF] </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.685.2640%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;ust=1477737172666000&amp;usg=AFQjCNFQzXXYmJZtbRAofJlg1CX_xhPLWw">Maximizing utilization in private iaas clouds with heterogenous load</a></span></h3><p class="c0"><span>T Vondra, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172667000&amp;usg=AFQjCNFkS5I5jsSmFwv9f5MghlF7Y2jHhQ">J Sedivy</a></span><span>&nbsp;- CLOUD COMPUTING, 2012 - Citeseer</span></p><p class="c0"><span>Abstract&mdash;This document presents ongoing work on creating a computing system that can </span></p><p class="c0"><span>run two types of workloads on a private cloud computing cluster, namely web servers and </span></p><p class="c0"><span>batch computing jobs, in a way that would maximize utilization of the computing ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D4050419507825584364%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172669000&amp;usg=AFQjCNF8DbigKo9gV8AaSWUZB9Xt0BzB8A">Cited by 3</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:7KhBExX7NTgJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172670000&amp;usg=AFQjCNEY3HWrndOqdO8Dn2ZMg6Kuqvk1Hw">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D4050419507825584364%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172671000&amp;usg=AFQjCNHpflhmBv3bOoL6u36FEpwClfof7Q">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D40%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172672000&amp;usg=AFQjCNE8fvOYdN_g6IRxXPAMX2h-wHNagw">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D40%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:IWHjjKOFINEC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172673000&amp;usg=AFQjCNEm3zcgsYTg-WssqnrJppjsdVoMfg">Saved</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D40%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172673000&amp;usg=AFQjCNFfF5mlR1rSjk6fgXjQlqWw8wP28g">More</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2667228/&amp;sa=D&amp;ust=1477737172674000&amp;usg=AFQjCNEYcSgauibyOIajTpR_TlCvFEf-Dg">[HTML] nih.gov</a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2667228/&amp;sa=D&amp;ust=1477737172675000&amp;usg=AFQjCNGiK-H8zdS-gWjs6AcAfcgogidZlQ"></a></span></p><h3 class="c0 c3" id="h.40v5qofbzexh"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://link.springer.com/chapter/10.1007/978-3-540-24837-8_15&amp;sa=D&amp;ust=1477737172676000&amp;usg=AFQjCNHKtMATEHhRVpUTI6uMGABmbMi6fw">Djinn: Interaction framework for home environment using speech and vision</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DxKxv70cAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172676000&amp;usg=AFQjCNGvFKbguphOLXC-9higZJHh2bo0aQ">J Kleindienst</a></span><span>, T Macek, L Ser&eacute;di, J &Scaron;ediv&yacute; - International Workshop on &hellip;, 2004 - Springer</span></p><p class="c0"><span>Abstract In this paper we describe an interaction framework that uses speech recognition </span></p><p class="c0"><span>and computer vision to model new generation of interfaces in the residential environment. </span></p><p class="c0"><span>We outline the blueprints of the architecture and describe the main building blocks. We ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D3754360951066567006%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172679000&amp;usg=AFQjCNFEC6cQ-jdVdKAfXtOGht540pE1Vg">Cited by 2</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:XqHrvmQrGjQJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172680000&amp;usg=AFQjCNGgklhbrQAclBzVxCBB3eZHrsU7ZQ">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D3754360951066567006%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172680000&amp;usg=AFQjCNFM0S5J4iOaKh7xYF7kinPAgahomQ">All 6 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D50%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172681000&amp;usg=AFQjCNFAl48PXz-OHjc49mCv-pJ0dS8sew">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D50%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172682000&amp;usg=AFQjCNFKCwuu6gCokk6ChAcz16FiOBFlTA">Save</a></span></p><h3 class="c0 c3" id="h.27suo3c0ghcg"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://dl.acm.org/citation.cfm?id%3D1181024&amp;sa=D&amp;ust=1477737172682000&amp;usg=AFQjCNFPmf5SBcq-EW9My0kgWJe8It8SYQ">CarDialer: multi-modal in-vehicle cellphone control application</a></span></h3><p class="c0"><span>&hellip;, M Fanta, M Labsk&yacute;, L Seredi, J &Scaron;ediv&yacute;&hellip; - Proceedings of the 8th &hellip;, 2006 - dl.acm.org</span></p><p class="c0"><span>Abstract This demo presents CarDialer-an in-car cellphone control application. Its multi-</span></p><p class="c0"><span>modal user interface blends state-of-the-art speech recognition technology (including text-to-</span></p><p class="c0"><span>speech synthesis) with the existing well proven elements of a vehicle information system ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D436573207991335321%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172684000&amp;usg=AFQjCNHwJ9z2LHJADGJ_hVx4Vxzk9bb_WQ">Cited by 2</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:mbnGBAUFDwYJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172685000&amp;usg=AFQjCNEI0KcbzI48dTBCk74qgBWLO6RioQ">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D436573207991335321%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172685000&amp;usg=AFQjCNExu7apcDtsBLI0vW-gkU6B_dhWFw">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D50%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172686000&amp;usg=AFQjCNGQUUbejlQbD9EaButs4Ivhyvsv9Q">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D50%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172687000&amp;usg=AFQjCNFPMGfvEj6OrtGRKSPnY17g40m_dA">Save</a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D50%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172687000&amp;usg=AFQjCNFPMGfvEj6OrtGRKSPnY17g40m_dA"></a></span></p><h3 class="c0 c3" id="h.sknyx5xfzyt0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US7805305&amp;sa=D&amp;ust=1477737172688000&amp;usg=AFQjCNFmwmk8E7lB9K-DzXdUcge0k7oNOg">Enhancement to Viterbi speech processing algorithm for hybrid speech models that conserves memory</a></span></h3><p class="c0"><span>&hellip;, T Beran, R Hampl, P Krbec, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172689000&amp;usg=AFQjCNGDiPN_O6H3e7fGmbEdnA6Gbttxmg">J Sedivy</a></span><span>&nbsp;- US Patent 7,805,305, 2010 - Google Patents</span></p><p class="c0"><span>The present invention discloses a method for semantically processing speech for speech </span></p><p class="c0"><span>recognition purposes. The method can reduce an amount of memory required for a Viterbi </span></p><p class="c0"><span>search of an N-gram language model having a value of N greater than two and also ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D12936455192289197173%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172690000&amp;usg=AFQjCNFzDceXQsdBEF1-i4xr_XA5G4I4Eg">Cited by 3</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:dbwnBXWFh7MJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172691000&amp;usg=AFQjCNEHIMhbw1tlWh3rKy8OwgHZEoKjgA">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D12936455192289197173%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172691000&amp;usg=AFQjCNHo7oadp8_t6wkdpfrP8on9qDPtrg">All 4 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D50%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172692000&amp;usg=AFQjCNFUUFX1fwMwMjOE2XYeRKB90i9DSA">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D50%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:Wp0gIr-vW9MC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172693000&amp;usg=AFQjCNHDI9vBUjyKHwkuK1Yc0FPNMvku9g">Saved</a></span></p><h3 class="c0 c3" id="h.noxs224kzl6c"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber%3D6354613&amp;sa=D&amp;ust=1477737172693000&amp;usg=AFQjCNEjNa9_WcoXooOwRYao-KyIx5Znow">MCSync-Distributed, Decentralized Database for Mobile Devices</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172694000&amp;usg=AFQjCNHf7IjJ8ajgAVEKNklmVNiZrE6GFg">J Sedivy</a></span><span>, T Barina, I Morozan&hellip; - Cloud Computing in &hellip;, 2012 - ieeexplore.ieee.org</span></p><p class="c0"><span>Abstract&mdash;A lot of research effort is being spent in the field of synchronization between </span></p><p class="c0"><span>mobile devices and cloud lately, but most of the applications use a centralized cloud storage </span></p><p class="c0"><span>solution. In contrast, Mobile Cloud Synchronization (MCSync) framework aims to give ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D17317962566502454447%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172696000&amp;usg=AFQjCNFnmKZDzwl27Rt1-enGM5p_SsBZow">Cited by 2</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:rxTkITPDVfAJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172696000&amp;usg=AFQjCNFRx0emmgYm3ZoZ34fO98axuFFbuA">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D17317962566502454447%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172697000&amp;usg=AFQjCNEPyAidPHXtB0UqINFkhVGDRa168A">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D50%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172698000&amp;usg=AFQjCNFXt6YJUr6sLNFS9gjuJ2WeHGIvQg">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D50%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:L8Ckcad2t8MC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172699000&amp;usg=AFQjCNFm1pdfNbi2x4bVYZNg3osG4LmF-A">Saved</a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D50%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:L8Ckcad2t8MC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172699000&amp;usg=AFQjCNFm1pdfNbi2x4bVYZNg3osG4LmF-A"></a></span></p><h3 class="c0 c3" id="h.zbx56x6wh3go"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://papers.sae.org/2006-01-0308/&amp;sa=D&amp;ust=1477737172700000&amp;usg=AFQjCNEf_rOxn-mZNaFgQqnUG4dIH8L5ZA">Advanced Development of Speech Enabled Voice Recognition Enabled Embedded Navigation Systems</a></span></h3><p class="c0"><span>K White, H Ruback, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172701000&amp;usg=AFQjCNHfi6QmrXw6KaiH_6QE8MeWArOaGw">J Sedivy</a></span><span>, K Kojima, K Kondo - 2006 - papers.sae.org</span></p><p class="c0"><span>Abstract: This paper will focus on the technology improvements and development effort </span></p><p class="c0"><span>required to enable human sounding system responses and voice enablement for destination </span></p><p class="c0"><span>entry for automotive navigation systems. Honda and IBM have been working together ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D2270728046028822252%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172702000&amp;usg=AFQjCNHdlTHbhbVKJgOOMZwju2j7XAdD4A">Cited by 1</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:7JYQhSA_gx8J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172703000&amp;usg=AFQjCNE2HIBGGcHwvmQHkdUvOOSzsUOorg">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D2270728046028822252%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172704000&amp;usg=AFQjCNG0ALYsavXfmeYtSDmlN-hgEQK3eQ">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D60%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172704000&amp;usg=AFQjCNEuus0aFK6IrQaoDCFRi9aUK8t3tA">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D60%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:MXK_kJrjxJIC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172705000&amp;usg=AFQjCNH2KP3PdKhywMZJeYzN4DhjxaLeuA">Saved</a></span></p><h3 class="c0 c3" id="h.v42xwqvuixk7"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US6584425&amp;sa=D&amp;ust=1477737172706000&amp;usg=AFQjCNGx8uMfo4lSWhlsxNV117qTm01U-w">Smart thermometer</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DhDnjeooAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172707000&amp;usg=AFQjCNF6kn-KkfaSuFRzO1-NcH-NCqQCxw">D Kanevsky</a></span><span>, M Sabath, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172707000&amp;usg=AFQjCNEtUZB4ecDanivLr90oRxxiFqf61Q">J Sedivy</a></span><span>, A Zlatsin - US Patent 6,584,425, 2003 - Google Patents</span></p><p class="c0"><span>A smart thermometer distributed system comprising a thermometer with a screen that allows </span></p><p class="c0"><span>one to enter a variety of data for the thermometer. The thermometer is connected to a </span></p><p class="c0"><span>computer and to a network and can retrieve from a history data base information about ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D8477545321469257566%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172709000&amp;usg=AFQjCNFvjhGfr_G4kSf008W9fXfc06Od8A">Cited by 1</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:XidzzY1KpnUJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172710000&amp;usg=AFQjCNEn8DQwaKaF-mRJ5b6StFHKnlewUA">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D8477545321469257566%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172711000&amp;usg=AFQjCNEHRvD8z1d7uPne3mruDOflPdbtng">All 4 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D60%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172712000&amp;usg=AFQjCNHH3J7W5-ysUirG1lgsu3HCuBKtRg">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D60%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:qxL8FJ1GzNcC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172712000&amp;usg=AFQjCNEdylOQrFZ6hH2bg7TPMvqEwCFt4Q">Saved</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ceur-ws.org/Vol-1391/131-CR.pdf&amp;sa=D&amp;ust=1477737172713000&amp;usg=AFQjCNGYbuIQlBwSoX8hPC_IdW3ndp0sjA">[PDF] ceur-ws.org</a></span></p><h3 class="c0 c3" id="h.m02rvbytafux"><span>[PDF] </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ceur-ws.org/Vol-1391/131-CR.pdf&amp;sa=D&amp;ust=1477737172714000&amp;usg=AFQjCNEyaW1H0Uuy9vX40_cQILNk-0nplQ">Biomedical Question Answering using the YodaQA System: Prototype Notes</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3Dj9OlBw0AAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172715000&amp;usg=AFQjCNGXTQdTb3Z4NSgPHz8gEZgBquo_pA">P Baudi&scaron;</a></span><span>, J &Scaron;ediv&yacute; - 2015 - ceur-ws.org</span></p><p class="c0"><span>Abstract. We briefly outline the YodaQA open domain question answering system and its </span></p><p class="c0"><span>initial adaptation to the Biomedical domain for the purposes of the BIOASQ challenge </span></p><p class="c0"><span>(question answering task 3b) on CLEF2015. Keywords: Question answering, linked data, ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D12106110844636465596%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172717000&amp;usg=AFQjCNEaKRugp_hQqrNAOldmQ0CSa7ehRg">Cited by 2</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:vIkPGqyLAagJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172717000&amp;usg=AFQjCNFWX3-xJQo0UzlUIWdH_-NN1XpWHg">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D12106110844636465596%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172718000&amp;usg=AFQjCNFL3UTxbbu8LK5_iMC_agdjQGg-HA">All 3 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D60%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172718000&amp;usg=AFQjCNFH3sSVdgf8R-aDbA8WokSBEq1npg">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D60%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172719000&amp;usg=AFQjCNGxAkLej6VyZZX4l45cprdgowj-nA">Save</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D60%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172720000&amp;usg=AFQjCNH7is5vR01klpMt3Sq1brdy3ceXig">More</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar_alerts?view_op%3Dcreate_alert_options%26hl%3Den%26alert_query%3D%2522jan%2Bsedivy%2522%26alert_params%3D%253Fhl%253Den%2526as_sdt%253D0,5&amp;sa=D&amp;ust=1477737172721000&amp;usg=AFQjCNG_S4wcZSurBdl-kkPfERa_5waVDw">Create alert</a></span></p><h3 class="c0 c3" id="h.8bb8649r35zg"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://www.google.com/patents/US7801727&amp;sa=D&amp;ust=1477737172721000&amp;usg=AFQjCNGixmGWIgzffKt-wkGncLdEjCGoyg">System and methods for acoustic and language modeling for automatic speech recognition with large vocabularies</a></span></h3><p class="c0"><span>&hellip;, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DhDnjeooAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172722000&amp;usg=AFQjCNE7TwNmBFr11VYrrkzxIqMIwZyXDA">D Kanevsky</a></span><span>, MD Monkowski, </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172723000&amp;usg=AFQjCNHHN3Cbhmk5MyzMkPab4GfU9i6wyg">J Sedivy</a></span><span>&nbsp;- US Patent &hellip;, 2010 - Google Patents</span></p><p class="c0"><span>A method for generating a language component vocabulary VC for a speech recognition </span></p><p class="c0"><span>system having a language vocabulary V of a plurality of word forms is disclosed. The method </span></p><p class="c0"><span>includes: partitioning the language vocabulary V into subsets of word forms based on ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D6896930610154736605%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172725000&amp;usg=AFQjCNGt1P7YbQl4-LSobu4Xb6Z6LncILw">Cited by 1</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:3YvKMtnRtl8J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172725000&amp;usg=AFQjCNESMAqDxDM2HMpVvVr9NBAR6SZb-Q">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D6896930610154736605%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172726000&amp;usg=AFQjCNEDzrSkuEipsOf28SfNVBrm9Z_UNA">All 4 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D70%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172727000&amp;usg=AFQjCNEefsLP7DcPyPt7gNSh3kdRUj1cKQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Djan%252Bsedivy%2526hl%253Den%2526start%253D70%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:M3ejUd6NZC8C%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172728000&amp;usg=AFQjCNFj-3OjK7qJ-cqKvsv_hzQoKYXR-Q">Saved</a></span></p><h3 class="c0 c3" id="h.1znabg6wm99x"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber%3D6722274&amp;sa=D&amp;ust=1477737172728000&amp;usg=AFQjCNF8VJbRR0-yaFp7WWp09ePWDZoKtw">Pairwise Learning to Rank for Search Query Correction</a></span></h3><p class="c0"><span>A Novak, J Sedivy - 2013 IEEE International Conference on &hellip;, 2013 - ieeexplore.ieee.org</span></p><p class="c0"><span>Abstract&mdash;This article introduces a new algorithm for a Search Query Spelling Correction </span></p><p class="c0"><span>System. It is based on learning to rank approach and allows to use large number of various </span></p><p class="c0"><span>signals leading to an improved accuracy. The performance will be tested against the ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D9626327862739267402%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172731000&amp;usg=AFQjCNFGeHTCqSNRnfnzvnypmtf2hIDWoQ">Cited by 1</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:Su8ETjCWl4UJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172731000&amp;usg=AFQjCNGBNWUwRFreEhxoiLK0W3uoE1dKfQ">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D9626327862739267402%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172732000&amp;usg=AFQjCNGZqQhkaBzq0dUOY7JNlCNZtoWX1Q">All 3 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D70%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172732000&amp;usg=AFQjCNEbP_4bq4-cbsv3w5vapZkWCznEWA">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D70%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172733000&amp;usg=AFQjCNExuEU_VJmtaAq5sTbU3x5pxgv7Vg">Save</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ajplung.physiology.org/content/308/1/L48.full&amp;sa=D&amp;ust=1477737172734000&amp;usg=AFQjCNFFHNwDU-ZfnzuYh5DZrvaoW1d7TQ">[HTML] physiology.org</a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ajplung.physiology.org/content/308/1/L48.full&amp;sa=D&amp;ust=1477737172734000&amp;usg=AFQjCNFFHNwDU-ZfnzuYh5DZrvaoW1d7TQ"></a></span></p><h3 class="c0 c3" id="h.wapmtdcfrmvp"><span>[PDF] </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://www.ivanecky.sk/Publikacie/tu-dresden2.pdf&amp;sa=D&amp;ust=1477737172735000&amp;usg=AFQjCNHeLteZEhDWWXZI4O4BUiNKdm7n7w">Towards multi-modal interfaces for embedded devices</a></span></h3><p class="c0"><span>V Fischer, C G&uuml;nther, J Ivaneck&yacute;, J &Scaron;ediv&yacute;&hellip; - &hellip; &ndash;Tagungsband der 13. &hellip;, 2002 - ivanecky.sk</span></p><p class="c0"><span>Abstract: The paper describes our efforts towards a multi&ndash;modal interface for embedded </span></p><p class="c0"><span>devices. Multi-modality is becoming more important especially in an embedded scenario </span></p><p class="c0"><span>where the&rdquo; standard&rdquo; ways of input (keyboard, mouse, stylus) as well as output (display) ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D5578371684038643715%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172737000&amp;usg=AFQjCNGJatwBSz4zmQU2QFMs-lR7fnQybw">Cited by 4</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:A5A4vH1bak0J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172738000&amp;usg=AFQjCNGagskHLvaUgO3nr6I3DVNjljBOxA">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D5578371684038643715%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172739000&amp;usg=AFQjCNFVqfrckJEfe6E2mSPS-8-Ki8EQzQ">All 3 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D70%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172740000&amp;usg=AFQjCNGg492WH-Z75THi_FE41dp7mm3xhw">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D70%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172741000&amp;usg=AFQjCNHHGrDVxU1LbK4mXph9uKyp8M3CQQ">Save</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D70%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172742000&amp;usg=AFQjCNEt0Z1S9ThilfTI8rXW5-dE-J3Y2w">More</a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D70%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172742000&amp;usg=AFQjCNEt0Z1S9ThilfTI8rXW5-dE-J3Y2w"></a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://onlinelibrary.wiley.com/doi/10.1111/j.1582-4934.2009.00931.x/full&amp;sa=D&amp;ust=1477737172743000&amp;usg=AFQjCNHOF6PoYC3hUdobW8lywLwtqmkpqA"></a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://onlinelibrary.wiley.com/doi/10.1111/j.1582-4934.2009.00931.x/full&amp;sa=D&amp;ust=1477737172744000&amp;usg=AFQjCNFwMCqHdx8ZZchafqDQkHExx-L-zQ"></a></span></p><p class="c0 c6"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?start%3D20%26q%3Djan%2Bsedivy%26hl%3Den%26as_sdt%3D0,5%23&amp;sa=D&amp;ust=1477737172744000&amp;usg=AFQjCNFU5L56c12WGgp9cBcyrPYGy4eIKA"></a></span></p><h3 class="c0 c3" id="h.5m061osxnhwu"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://link.springer.com/chapter/10.1007/978-3-319-24027-5_20&amp;sa=D&amp;ust=1477737172745000&amp;usg=AFQjCNHgZ813wtpnW7rW6ezNc78w7XVWeA">Modeling of the question answering task in the YodaQA system</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3Dj9OlBw0AAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172746000&amp;usg=AFQjCNG7Q5d-TnI5EKM7MGPCWxlkyCGUew">P Baudi&scaron;</a></span><span>, J &Scaron;ediv&yacute; - International Conference of the Cross-Language &hellip;, 2015 - Springer</span></p><p class="c0"><span>Abstract We briefly survey the current state of art in the field of Question Answering and </span></p><p class="c0"><span>present the YodaQA system, an open source framework for this task and a baseline pipeline </span></p><p class="c0"><span>with reasonable performance. We take a holistic approach, reviewing and aiming to ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D6773279035442428961%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172748000&amp;usg=AFQjCNEUoUYr5i1lTo2mtZ-RLFIzZ9TUCA">Cited by 5</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:IXwleGWF_10J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172749000&amp;usg=AFQjCNGfj47VtyDu2W54lpZhC2lzqjuzNw">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D6773279035442428961%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172750000&amp;usg=AFQjCNE766gz8jOq-xulrLqHoTQq88Uhpw">All 5 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172750000&amp;usg=AFQjCNHl_xotGBvNESwaEC75YidLHBXXkw">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172751000&amp;usg=AFQjCNFtNGyoYyw2jyUAUZAAhimwLQoz8w">Save</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ailao.eu/yodaqa/yodaqa-poster2015.pdf&amp;sa=D&amp;ust=1477737172752000&amp;usg=AFQjCNHWLXIa12zvOqBurJl8zEjtnrVRBw">[PDF] ailao.eu</a></span></p><h3 class="c0 c3" id="h.eg1uptlk5q9d"><span>[PDF] </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ailao.eu/yodaqa/yodaqa-poster2015.pdf&amp;sa=D&amp;ust=1477737172753000&amp;usg=AFQjCNHALh3LkyIJ1VdHIjlTRjrbhLjVoQ">YodaQA: a modular question answering system pipeline</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3Dj9OlBw0AAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172754000&amp;usg=AFQjCNG7Wr0nrNHglJppCO-amUs0jBAKhw">P Baudi&scaron;</a></span><span>&nbsp;- POSTER 2015-19th International Student Conference &hellip;, 2015 - ailao.eu</span></p><p class="c0"><span>Abstract. This is a preprint, submitted on 2015-03-22. Question Answering as a sub-field of </span></p><p class="c0"><span>information retrieval and information extraction is recently enjoying renewed popularity, </span></p><p class="c0"><span>triggered by the publicized success of IBM Watson in the Jeopardy! competition. But ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D3330913485599629992%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172756000&amp;usg=AFQjCNHHfpB7M1UYdl6-ZJZfC-LTKQfLFQ">Cited by 10</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:qGK5zirIOS4J:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172756000&amp;usg=AFQjCNHT5rh0b5BjFwiBQSgRZSIX0H04Aw">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D3330913485599629992%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172757000&amp;usg=AFQjCNFGXSDST7MSUTN9nfO3HHNralTD_w">All 5 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172758000&amp;usg=AFQjCNElo64I986CxJrZ5aDoNRSTtUm07w">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172758000&amp;usg=AFQjCNElo64I986CxJrZ5aDoNRSTtUm07w">Save</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172759000&amp;usg=AFQjCNFnGiefmyF-9zBisjkMpFQIUceJ5g">More</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ceur-ws.org/Vol-1391/131-CR.pdf&amp;sa=D&amp;ust=1477737172760000&amp;usg=AFQjCNFlh94Oj5dkFA_lBU1YsE2C4oK6lA">[PDF] ceur-ws.org</a></span></p><h3 class="c0 c3" id="h.dkdjjeruo0cm"><span>[PDF] </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ceur-ws.org/Vol-1391/131-CR.pdf&amp;sa=D&amp;ust=1477737172760000&amp;usg=AFQjCNFlh94Oj5dkFA_lBU1YsE2C4oK6lA">Biomedical Question Answering using the YodaQA System: Prototype Notes</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3Dj9OlBw0AAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172761000&amp;usg=AFQjCNEtsWtJfSmjB4MMlUya2YZEzXHYEg">P Baudi&scaron;</a></span><span>, J &Scaron;ediv&yacute; - 2015 - ceur-ws.org</span></p><p class="c0"><span>Abstract. We briefly outline the YodaQA open domain question answering system and its </span></p><p class="c0"><span>initial adaptation to the Biomedical domain for the purposes of the BIOASQ challenge </span></p><p class="c0"><span>(question answering task 3b) on CLEF2015. Keywords: Question answering, linked data, ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D12106110844636465596%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172763000&amp;usg=AFQjCNFI4K1bMSkKsDh2y5f2_TnscoG_qg">Cited by 2</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:vIkPGqyLAagJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172764000&amp;usg=AFQjCNF5vNplQclWfFbqrkjpO20tQ4F8MQ">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D12106110844636465596%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172765000&amp;usg=AFQjCNEWKWofiGgLxDG4loj8JocyDE8P0Q">All 3 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172765000&amp;usg=AFQjCNGl_ADJ8OVncTLPxhwaxccjrg-KlQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172766000&amp;usg=AFQjCNHxrNkAhSFpwdWRCFjop5dfPlDzLQ">Save</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172767000&amp;usg=AFQjCNFtv-UtA889_oDAeaTTy2jxNtWp0Q">More</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://arxiv.org/pdf/1605.04655&amp;sa=D&amp;ust=1477737172767000&amp;usg=AFQjCNFCMnSjf__gXjbTddOXXL0U3DT4kg">[PDF] arxiv.org</a></span></p><h3 class="c0 c3" id="h.r1mknrbpheua"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://arxiv.org/abs/1605.04655&amp;sa=D&amp;ust=1477737172768000&amp;usg=AFQjCNE9l0xFrKi-iOvKW1_4Rt5j5IMCNQ">Joint Learning of Sentence Embeddings for Relevance and Entailment</a></span></h3><p class="c0"><span>P Baudis, S Stanko, J Sedivy - arXiv preprint arXiv:1605.04655, 2016 - arxiv.org</span></p><p class="c0"><span>Abstract: We consider the problem of Recognizing Textual Entailment within an Information </span></p><p class="c0"><span>Retrieval context, where we must simultaneously determine the relevancy as well as degree </span></p><p class="c0"><span>of entailment for individual pieces of evidence to determine a yes/no answer to a binary ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D6304437634013785405%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172770000&amp;usg=AFQjCNH9vhCoAMloXHMswT3z0vwNSNLctQ">Cited by 1</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:Pe3-M6DcfVcJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172771000&amp;usg=AFQjCNEwyDGO2PJA_3xeU9ouOhXk3Fjn9Q">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D6304437634013785405%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172771000&amp;usg=AFQjCNERx_YOPBY0lpWkj5saI3T1IKclUg">All 2 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172772000&amp;usg=AFQjCNEQ8oVxldGaA6dxLYFIXSF2Xcs7QQ">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172773000&amp;usg=AFQjCNHfH-JYX5ICBysji5rEhuu_mOrtEw">Save</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://arxiv.org/pdf/1603.06127&amp;sa=D&amp;ust=1477737172773000&amp;usg=AFQjCNFHBX-_WMKS0A1JLwUHAUeNDJInXQ">[PDF] arxiv.org</a></span></p><h3 class="c0 c3" id="h.pv817odpwddj"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06127&amp;sa=D&amp;ust=1477737172774000&amp;usg=AFQjCNHLHg2gd3OspLrUrJlbhfmNDylxYg">Sentence Pair Scoring: Towards Unified Framework for Text Comprehension</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3Dj9OlBw0AAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172775000&amp;usg=AFQjCNH3J6H143JgrnjwLJs1pPOVCsVCHA">P Baudi&scaron;</a></span><span>, J &Scaron;ediv&yacute; - arXiv preprint arXiv:1603.06127, 2016 - arxiv.org</span></p><p class="c0"><span>Abstract: We review the task of Sentence Pair Scoring, popular in the literature in various </span></p><p class="c0"><span>forms---slanted as Answer Sentence Selection, Paraphrasing, Semantic Text Scoring, Next </span></p><p class="c0"><span>Utterance Ranking, Recognizing Textual Entailment or eg a component of Memory ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cites%3D10096876682197008241%26as_sdt%3D2005%26sciodt%3D0,5%26hl%3Den&amp;sa=D&amp;ust=1477737172777000&amp;usg=AFQjCNFVKZqqdRtepFHNU1dwJG67qz22pA">Cited by 2</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:cf9Gz9hPH4wJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172778000&amp;usg=AFQjCNHOzl_OguTqMvorBMG4DsEhIPaYpg">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?cluster%3D10096876682197008241%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172779000&amp;usg=AFQjCNHmfWaeAo-3P1DYi2ZQfkIuqYNq0Q">All 4 versions</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172779000&amp;usg=AFQjCNFDRpEMF6bt8S16JcJLWyh2MHJRtg">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172780000&amp;usg=AFQjCNHaA-RXJYTU1Z9Sjm3Q3i2HHdKXKA">Save</a></span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ailao.eu/yodaqa/yodaqa-clef2015-qald.pdf&amp;sa=D&amp;ust=1477737172780000&amp;usg=AFQjCNFd_Y2S5ascDpHejNvh4Qs7XT1spw">[PDF] ailao.eu</a></span></p><h3 class="c0 c3" id="h.xfr4crvupixw"><span>[PDF] </span><span class="c1"><a class="c2" href="https://www.google.com/url?q=http://ailao.eu/yodaqa/yodaqa-clef2015-qald.pdf&amp;sa=D&amp;ust=1477737172781000&amp;usg=AFQjCNG06Zvwz6VwJoaMehQkMfrgJUFqmA">QALD Challenge and the YodaQA System: Prototype Notes</a></span></h3><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3Dj9OlBw0AAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172782000&amp;usg=AFQjCNEtLXAJ4g1S6qEX2enKy-IFRr8i7w">P Baudi&scaron;</a></span><span>, J &Scaron;ediv&yacute; - ailao.eu</span></p><p class="c0"><span>Abstract. We briefly outline the YodaQA open domain question answering system and its </span></p><p class="c0"><span>initial application on the Question Answering over Linked Data challenge QALD5 on </span></p><p class="c0"><span>CLEF2015. Since YodaQA is focused on QA over unstructured data and has been only ...</span></p><p class="c0"><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Drelated:ocV_84QKyFIJ:scholar.google.com/%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;ust=1477737172783000&amp;usg=AFQjCNET3iZOayQiA9iBiUk0rtNDLS_Lwg">Related articles</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172784000&amp;usg=AFQjCNGb-sx87FZ3oNlM52JsM5efGrvotA">Cite</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172785000&amp;usg=AFQjCNHMVKBoOO_Hj5DdiCU5puLE65YK5Q">Save</a></span><span>&nbsp;</span><span class="c1"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172786000&amp;usg=AFQjCNGEw4gBUC7gPoXgc9EQeoWTi31-7g">More</a></span></p><p class="c0 c6"><span></span></p><h3 class="c0 c25" id="h.t29ihtkn373n"><span class="c13"><a class="c2" href="https://www.google.com/url?q=http://link.springer.com/chapter/10.1007/978-3-319-44944-9_14&amp;sa=D&amp;ust=1477737172787000&amp;usg=AFQjCNEMUc_BQPECRcglaZEIk9uSjKVSxQ">Deep Neural Networks for Web Page Information Extraction</a></span></h3><p class="c0 c22"><span class="c8">T </span><span class="c8 c23">Gogar</span><span class="c8">, O Hubacek, </span><span class="c8 c20"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172788000&amp;usg=AFQjCNFbHdtNXayfDNh9xm3HJSUFG9vdbw">J </a></span><span class="c8 c16"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?user%3DDwwD3MkAAAAJ%26hl%3Den%26oi%3Dsra&amp;sa=D&amp;ust=1477737172789000&amp;usg=AFQjCNFYTV-xnOPddwg4rr6VtWU-ZUf19Q">Sedivy</a></span><span class="c8">&nbsp;- IFIP International Conference on Artificial &hellip;, 2016 - Springer</span></p><p class="c0 c7"><span class="c15">Abstract Web wrappers are systems for extracting structured information from web pages. </span></p><p class="c7 c0"><span class="c15">Currently, wrappers need to be adapted to a particular website template before they can </span></p><p class="c7 c0"><span class="c15">start the extraction process. In this work we present a new method, which uses ...</span></p><p class="c0 c10"><span class="c11"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dtomas%2Bgogar%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172790000&amp;usg=AFQjCNFrZDymqhlljmZhZH1EmERBNFuTuA">Cite</a></span><span class="c26">&nbsp;</span><span class="c11"><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/citations?view_op%3Dview_citation%26continue%3D/scholar%253Fq%253Dtomas%252Bgogar%252Bjan%252Bsedivy%2526hl%253Den%2526as_sdt%253D0,5%26citilm%3D1%26citation_for_view%3DDwwD3MkAAAAJ:4xDN1ZYqzskC%26hl%3Den%26oi%3Dsaved&amp;sa=D&amp;ust=1477737172791000&amp;usg=AFQjCNHKkE0ECrqHRhbsBtm7i-2HImUhMA">Saved</a></span></p><p class="c0 c6"><span><a class="c2" href="https://www.google.com/url?q=https://scholar.google.cz/scholar?q%3Dpetr%2Bbaudis%2Bjan%2Bsedivy%26btnG%3D%26hl%3Den%26as_sdt%3D0%252C5%23&amp;sa=D&amp;ust=1477737172792000&amp;usg=AFQjCNF-Y_EJSJYJLgXhnYDIe6NT0qEl9Q"></a></span></p><p class="c0 c6"><span></span></p></body></html>